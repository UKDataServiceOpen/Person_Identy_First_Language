{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71e8f5f",
   "metadata": {},
   "source": [
    "# Word frequencies\n",
    "\n",
    "\n",
    "Following the first code notebook, we now have the texts from all abstracts abstracts in two nice neat .csv files. One with all of the abstracts that we were able to identify and one with only the subset of the identified abstracts that also contain one or more of the keywords of interest. \n",
    "\n",
    "This second notebook starts with downloading and importing the packages we need, importing the .csv files, and moving on to then starting the first part of the analysis. \n",
    "\n",
    "## Get ready \n",
    "\n",
    "As always, we start with a couple of code cells that load up and nickname some useful packages, then check file locations, then import files and check them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26078126",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# installing necessary pdf conversion packages via pip\n",
    "# the '%%capture' at the top of this cell suppresses the output (which is normally quite long and annoying looking). \n",
    "# You can remove or comment it out if you prefer to see the output. \n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "import nltk                       # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "from nltk import word_tokenize    # and some of its key functions\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.corpus import wordnet                    # Finally, things we need for lemmatising!\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "nltk.download('averaged_perceptron_tagger')        # Like a POS-tagger...\n",
    "nltk.download('wordnet')\n",
    "nltk.download('webtext')\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "import numpy as np\n",
    "import statistics\n",
    "import datetime\n",
    "date = datetime.date.today()\n",
    "\n",
    "import codecs\n",
    "import csv                        # csv is for importing and working with csv files\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import statistics\n",
    "import re                         # things we need for RegEx corrections\n",
    "import matplotlib.pyplot as plt\n",
    "import string \n",
    "\n",
    "import math \n",
    "\n",
    "English_punctuation = \"-!\\\"#$%&()'*-–+,./:;<=>?@[\\]^_`{|}~''“”\"      # Things for removing punctuation, stopwords and empty strings\n",
    "table_punctuation = str.maketrans('','', English_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef24df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"..\\\\output\")  )                                # check 'results' folder is not empty/has correct stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e364e65f",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "Having checked the contents of the output folder and seen the files we expected to see, we can now import and check them. \n",
    "There is an additional file here, called 'ESHG_abstract_sumbissions.csv' which contains numbers given to us my the ESHG conference team about how many abstracts they accepted each year. This will be useful for comparing to how many abstracts we detected each year to get a sense of how successful our process is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076b8611",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = pd.read_csv('..\\\\output\\\\all_abstracts_no_null_texts.csv')            # one for all of the texts and then\n",
    "matched_texts = pd.read_csv('..\\\\output\\\\matched_abstracts_no_null_texts.csv')    # one for just those that match the keyword\n",
    "reported_abstracts = pd.read_csv('..\\\\output\\\\ESHG_abstract_submissions.csv')     # one for the number of expected abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(all_texts))                        # it is always useful to double check that the length matches your expectations\n",
    "print (len(matched_texts))                    # in this case, we already know how many rows to expect in each file. \n",
    "print (len(reported_abstracts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d22b75",
   "metadata": {},
   "source": [
    "## Get some basic stats about how texts are spread out over time\n",
    "\n",
    "We know that all of the rows in the files have at least two columns with contents - 'Year' and 'Text'. This means that it is probably a useful thing to get a little schematic and/or table that counts row according to year. Let's do that now!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc03bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counts_by_year = all_texts['Year'].value_counts()         # this creates a little table with two columns - year and count\n",
    "print(all_counts_by_year)                                     # however, when we print it we can see it has no headers,\n",
    "                                                              # is not in order, has the years appearing as  floats, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0be49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counts_by_year = pd.DataFrame(all_counts_by_year)                            # Convert the imported table to a data frame,\n",
    "all_counts_by_year = all_counts_by_year.rename(columns={\"Year\": \"All\"})          # rename the columns,\n",
    "all_counts_by_year = all_counts_by_year.rename_axis('Year').reset_index()        # set the axis to 'Year' and reset the index,\n",
    "all_counts_by_year = all_counts_by_year.sort_values(by=['Year']).astype('Int64') # retype 'Year' column and sort by year.\n",
    "print(all_counts_by_year)                                                        # Let's just check it worked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039a408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_counts_by_year = matched_texts['Year'].value_counts()                             # repeat for the matched_texts file\n",
    "matched_counts_by_year = pd.DataFrame(matched_counts_by_year)                             # again, turn it into a data frame\n",
    "matched_counts_by_year = matched_counts_by_year.rename(columns={\"Year\": \"Matches\"})       # name the columns\n",
    "matched_counts_by_year = matched_counts_by_year.rename_axis('Year').reset_index()         # rename axis, reset index\n",
    "matched_counts_by_year = matched_counts_by_year.sort_values(by=['Year']).astype('Int64')  # retype and sort by value of year\n",
    "print(matched_counts_by_year)                                                             # and check it looks correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda26ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reported_abstracts             # This file wasn't created by our preparation processes, so we check it. \n",
    "                               # We don't need all the columns, just 'Year' and 'Total'. \n",
    "                               # Also, we can see that we don't need to retype the 'Year' or sort by 'Year' value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5baeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reported_abstracts_by_year = pd.DataFrame(reported_abstracts[['Year', 'Total']])            # create data frame from just 2 columns\n",
    "reported_abstracts_by_year = reported_abstracts_by_year.rename(columns = {'Total':'Reported'}) # rename 'Total' column to 'Expected'\n",
    "print(reported_abstracts_by_year)                                                           # Let's just check it worked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85569dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_year = all_counts_by_year.merge(matched_counts_by_year, on='Year', how='left') # now, combine the 1st two data frames\n",
    "counts_year = counts_year.merge(reported_abstracts_by_year, on='Year', how='left')       # add the 3rd \n",
    "print(counts_year)                                                                    # and have a look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb16471",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_year['Year'] = pd.to_datetime(counts_year['Year'].astype(str), format=\"%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dd4dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_year = counts_year.set_index('Year')                     # set the year as the index\n",
    "print(counts_year)                                              # and have a look. Nice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts_year['All'].sum())                                  # I worry too much sometimes\n",
    "print(counts_year['Matches'].sum())                              # but why not check that the numbers still add up? \n",
    "print(counts_year['Reported'].sum())                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100df155",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_year.plot()                          # create a plot from the combined, re-indexed and renamed data frame\n",
    "plt.legend(frameon=False)         # Set position for legend and set legend frame to be false\n",
    "plt.show()                                          # have a look at the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2cbb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('..\\\\output\\\\abstract_count.jpg')    # we can right click on the plot above to save it, or save it via command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a4c1c",
   "metadata": {},
   "source": [
    "## Count word frequencies - 'bag of words'\n",
    "\n",
    "Now that we have some basic descriptive stats about how many abstracts were imported properly with text in the 'Text' column, we can get on to the actual natural language processing steps. The most basic NLP option is to count the most frequent words found in the two sets of abstracts - meaning we need to find the most frequent words found in ALL of the abstracts and then compare that to the most frequnet words found in only those abstracts that contain a keyword of interest. \n",
    "\n",
    "To this end, we use the 'bag of words' method which whacks all of the words from all of the texts together, turns them into 'tokens' then processes to make them as unified as possible by removing uppercase letters, punctuation, digits, empty strings, stop words (e.g. 'the', 'and', 'for', etc. ) and word forms (e.g. pluralisations, verb endings, etc. ). \n",
    "\n",
    "Let's demo this with a simple example. If the text we want to 'bag of words' is \"The cat named Cat was one of 5 cats.\" it would become a list of stemmed word-tokens like \n",
    "'''[[cat]\n",
    "[name]\n",
    "[cat]\n",
    "[be]\n",
    "[cat]]''' \n",
    "and the most common word would obviously be '''[cat]'''. \n",
    "\n",
    "Applying the 'bag of words' method to our texts is not so trivial, but should also be more enlightening. We would expect that the most common words from all of the texts would be similar to, but not identical to, the most common words from only the abstracts that contain a keyword of interest.\n",
    "\n",
    "This bag of words approach ignores years, session codes, authors and everything else. Subsetting the texts by those things might be useful later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16684bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_analysis(input, how_many):     # define a 'bag of words' function with 2 arguments, an input and a quantity \n",
    "    holding_string = \"\"                                                        # that creates a temporary variable\n",
    "    for text in input['Text']:                                                 # looks at the 'Text' column for the input\n",
    "        holding_string += text                                                 # fills up the temp variable with the text\n",
    "    holding_string = word_tokenize(holding_string)                             # word tokenises that text\n",
    "    holding_string = [word.lower() for word in holding_string]                 # remove uppercase letters\n",
    "    holding_string = [w.translate(table_punctuation) for w in holding_string]  # removes punctuation\n",
    "    holding_string = (list(filter(lambda x: x, holding_string)))               # removes andy empty strings\n",
    "    holding_string = [token for token in holding_string if not token.isdigit()]  # removes digits\n",
    "    holding_string = [token for token in holding_string if token not in stop_words]  # removes stopwords\n",
    "    holding_string = [porter.stem(token) for token in holding_string]                # stems the word-tokens\n",
    "    list_for_count = []                                                              # and creates an empty list\n",
    "    for token in holding_string:                                         # then iterates over the tokens\n",
    "        list_for_count.append(token)                                     # appending them to the list\n",
    "    counts = Counter(list_for_count)                                     # applies the Counter function imported earlier \n",
    "    return counts.most_common(how_many)                                  # and returns the tokens with highest counts \n",
    "                                                                         # up to the quantity specified as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_all = bag_of_words_analysis(all_texts, 20)   # apply bag of words function to all texts, and save the output table\n",
    "                                                           # this will take a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575241cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_all = pd.DataFrame(most_frequent_all)                                # convert the saved output as a data frame\n",
    "most_frequent_all = most_frequent_all.rename(columns={0: \"Word\", 1: \"All count\" }) # name the columns\n",
    "print(most_frequent_all)                                                  # Let's just check it worked. print(most_frequent_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3f528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_matched = bag_of_words_analysis(matched_texts, 20) # apply the bag of words function to matched texts, and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5882b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_matched = pd.DataFrame(most_frequent_matched)                                    # convert it to a data frame\n",
    "most_frequent_matched = most_frequent_matched.rename(columns={0: \"Word\", 1: \"Matched count\" }) # name the columns\n",
    "print(most_frequent_matched)                                                  # Let's just check it worked. print(most_frequent_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7474420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent = most_frequent_all.merge(most_frequent_matched, on='Word', how='outer') # combine the two data frames via 'outer'\n",
    "                                                                                       # to get the total list of all words\n",
    "print(most_frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a66fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent.to_csv('..\\\\output\\\\most_frequent_comparison.csv')  # write out the joined most frequent words to a .csv\n",
    "                                                                  # again, with a clear and useful name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
