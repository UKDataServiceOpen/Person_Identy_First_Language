{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b46e5d4f",
   "metadata": {},
   "source": [
    "# Step 1 - Prepare\n",
    "\n",
    "Admittedly, quite a bit of work has already taken place. The original .pdf files were sent off to a research support team that scraped the text out of them and stored the contents of those files in .csv files which have useful headers. Each .pdf file was processed to produce a single .csv file, which you will find in 'root/2023_Second_analysis/results/' . The original .pdf files can be found in the 'root/2022_First_analysis/input_pdfs/ESHG/', but since there is no code in this second attempt that uses the original .pdfs directly, they are not included here too. \n",
    "\n",
    "This processing was not universally successful; the original .pdf files were not all encoded in the same format and most had a totally unique layout. In effect, this means that each .pdf-to-.csv process was not equally able to automatically capture and segment the text in the .pdf to turn it into .csv columns such as author, affiliation, session code, etc. This means that some of the .csv files have empty columns while other .csv files have contents in those columns. This also means that some of the rows are probably faulty. Obviously, a detailed manual inspection would correct some of these errors but total accuracy is not the point of this research effort. \n",
    "\n",
    "The file for 2004 was particularly tricky and needs separate attention because the output from the research support team was not structured in a way that matches the others very well. \n",
    "\n",
    "Nevertheless, the output from the research support team is a set of files that are useful for natural language processing methods to investigate person-first and identity-first language. The next step is to import the various .csv files, consolidate them in one data frame, tidy up some of the erroneous rows and columns and then save the output in a new .csv. \n",
    "\n",
    "## Get ready\n",
    "\n",
    "All of my jupyter notebooks begin with some code cells focussed on downloading/importing necessary packages, loading useful short names, and so forth. \n",
    "\n",
    "I also like to check the relevant file locations before importing the .csv files to work on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture                         \n",
    "                                  # The above capture statement is optional. \n",
    "                                  # You can remove this to see the chatter normally produced during import steps. \n",
    "\n",
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "\n",
    "import pandas as pd               # pandas is necessary for working with data frames - shortening it to pd just saves time. \n",
    "pd.set_option('display.max_colwidth', 200)   # some of the files are big so set a big column width. \n",
    "import numpy as np                # like pandas, numpy is useful and useful to have a short name for\n",
    "import statistics                 # gotsta have stats\n",
    "\n",
    "import csv                        # csv is for importing and working with csv files\n",
    "\n",
    "import re                         # things we need for RegEx corrections\n",
    "import string \n",
    "import math \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88df952",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "Having got all the packages we need and having checked the files, let's import them. This requires:\n",
    "* defining a function to import multiple files from a known location (better than one-by-one importing!)\n",
    "* checking the output of the mass-import for length and contents (since I suspect 2004 may not have worked correctly)\n",
    "* having found an error, investigating it a bit\n",
    "* correcting the error by removing problem rows, manually incorporating better rows, adding correct rows back in and checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da8258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"..\\\\results\")  )                                # check 'results' folder is not empty/has correct stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c6ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []                                                        # create empty list to hold names of files in 'results'\n",
    "def import_results(input):                                        # create a function import the contents of the\n",
    "    for f in os.listdir(input):                                   # folder named in the function input\n",
    "        f = pd.read_csv(input + '\\\\'+ f,encoding='latin1')        # by reading them in as csv files, one by one\n",
    "        files.append(f)                                           # appending the newly read csv file to a temporary list\n",
    "    output = pd.concat(files)                                     # then concatenating that temp list to the pre-defined list\n",
    "    return output                                                 # returning the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b74ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = import_results(\"..\\\\results\")      # run the newly defined function on the 'results' folder\n",
    "how_many_total = len(all_results)                # check the length \n",
    "how_many_total                                   # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10626120",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_results['Year'].drop_duplicates())     # quick check shows that 2004 (a known problem file) has not imported properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc45084",
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_no_year = all_results['Year'].isna().sum() # Let's just count how many rows NaN instead of the year\n",
    "how_many_no_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_Nan_in_Year = all_results[~all_results['Year'].isnull()]          # remove the 'Year' = Nan rows\n",
    "how_many_without_Nan_year = len(no_Nan_in_Year)                      # check length again now that Nan rows are nemoved\n",
    "how_many_without_Nan_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c7cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (how_many_total - how_many_no_year )                   # print the total number minus the number that are missing a year\n",
    "print (how_many_without_Nan_year)                            # compare to the new total just to be sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163269ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_04 = pd.read_csv('..\\\\results\\\\ESHG2004.csv')      # specifically read in year 2004 (it needed a bit of extra work)\n",
    "year_04 = year_04.iloc[:, [0,1]]                        # cut a two-column slice out of it with only the year and text\n",
    "year_04                                                 # check how it looks (which also shows us how many rows are in it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_corrected = pd.concat([no_Nan_in_Year, year_04])     # add those specially imported 2004 rows back into the output\n",
    "                                                                 # which had the weird no-year rows removed\n",
    "    \n",
    "print(len(all_results_corrected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7790cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_total_new = len(all_results_corrected)                  # check length again - are we back up to where we started?\n",
    "\n",
    "if how_many_total == how_many_total_new :                        # write a quick check to be sure the totals before and after\n",
    "    print('The numbers add up!')                                 # removing/replacing the 2004 rows are the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0560fad",
   "metadata": {},
   "source": [
    "## Remove rows that imported correctly for reasons unrelated to 2004. \n",
    "\n",
    "Having imported all the various .csv files and storing them in one data frame (even that tricksy 2004 .csv) I do a bit of clean up. Turning .pdf files to .csv is not a straightforward or fool proof process, so I want to remove any rows that have nothing in the 'Text' column and check the length again to see how many we have lost. Then I want to remove any columns that are entirely empty (which is probably the result of badly imported rows that are shifted over) and have a quick look at the remaining columns and what might be in them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a5f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_texts = all_results_corrected[~all_results_corrected['Text'].isnull()] \n",
    "                                                                    # remove any rows where the 'text' column is empty\n",
    "how_many_no_null_texts = len(no_null_texts)                         # check length again - still making sense?\n",
    "print (how_many_no_null_texts)\n",
    "print (how_many_total_new - how_many_no_null_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddac587",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_texts = no_null_texts.dropna(axis=1, how=\"all\")   # remove all columns which contain only NaNs\n",
    "print(len(no_null_texts))                                 # just check the length has not changed\n",
    "no_null_texts                                             # have a nosy at which columns remain, what it is them, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d103a0",
   "metadata": {},
   "source": [
    "## Clean up the 'Text' column a little bit\n",
    "\n",
    "It was not obvious what text cleaning steps were most valuable when we first started working with this text data. However, having run through all of the steps several times, I have concluded that a few cleaning steps at this early stage are very useful. The first of these does basic things like removing multiple sequential whitespaces and inserting spaces when two sentences have been jammed together. \n",
    "\n",
    "Following that, another cleaning step standardises the keywords of interest. For example, this would turn 'Asperger's syndrome', 'Asperger syndrome disorder', 'autistic spectrum disorder' and 'autism spectrum disorder' into 'ASD'.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d56d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spacing_errors (input):\n",
    "    no_extra_spaces = re.sub(r'(\\s)(\\s+)', r'\\1', input)                        # turn 2+ sequential whitespaces into 1\n",
    "    no_run_ons1 = re.sub(r'([a-z][.|?|;])([A-Z])', r'\\1 \\2', no_extra_spaces)   # removes run-ons (e.g. \"word.New sentence \")\n",
    "    no_run_ons2 = re.sub(r'([A-Z][.|?|;])([A-Z])', r'\\1 \\2', no_run_ons1)       # removes run-ons (e.g. \"ACRONYM.New sentence \")\n",
    "    space1 = re.sub(r'([a-z]+)(disorder|disability|spectrum|disease)', r'\\1 \\2', no_run_ons2) # adds a space in select run-ons\n",
    "    space2 = re.sub(r'(spec) (trum)', r'\\1\\2', space1)                          # removes a space between 'spec' and 'trum'\n",
    "    space3 = re.sub(r'(psychopa) (thy)', r'\\1\\2', space2)                          # removes a space between 'spec' and 'trum'\n",
    "\n",
    "    return(space3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e357be",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Optional cell code block to test or understand what the remove_spacing_errors function does\n",
    "    \n",
    "spacing_error_test = \"word.New sentence   extra spaces    test. \\\n",
    "            ACRONYM.New sentence    \\\n",
    "            Asperger'sdisorder sdisorder Autisticdisability autismspectrum \\\n",
    "            autismdisease spec trum \"\n",
    "\n",
    "remove_spacing_errors(spacing_error_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43395654",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_run_ons = [remove_spacing_errors(abstract) for abstract in no_null_texts['Text'] ] \n",
    "                                             # create list of texts without extra spaces/run-on erors \n",
    "                                             # this is to improve sentence tokenisation later \n",
    "no_null_texts['Text'] = no_run_ons           # Overwrite the 'Text' column with the new no extra space/run-on abstract list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa834092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_up_terminology (input):\n",
    "    no_apost = re.sub(r'([Aa]sperger)(\\'?)(s?)', r'asperger', input)   # Standardises case and apostrophe in '[Aa]sperger's'\n",
    "    lower1 = re.sub(r'[Ss]pectrums|[Ss]pectra|[Ss]pectrum', r'spectrum', no_apost) # lowercases / removes plurals for spectrum\n",
    "    lower2 = re.sub(r'[Ss]yndromes|[Ss]yndrome', r'syndrome', lower1)              # lowercases / removes plurals for syndrome\n",
    "    lower3 = re.sub(r'[Dd]isorders|Disorder', r'disorder', lower2)                 # lowercases / removes plurals for disorder\n",
    "    lower4 = re.sub(r'[Dd]iseases|Disease', r'disease', lower3)                    # lowercases / removes plurals for disease\n",
    "    lower5 = re.sub(r'[Aa]utis', r'autis', lower4)                                  # lowercases 'Autism' and 'Autistic'\n",
    "    plur = re.sub(r'ASD([\\'*?])(s?)', r'ASD', lower5)                            # removes plural for more than one ASD\n",
    "    aut0 = re.sub(r'(asperger )(autis)', r'\\2', plur)                  # removes '[Aa]sperger' from '[Aa]sperger' autis'\n",
    "    aut1 = re.sub(r'(autistic )(psychopathy)', r'autism', aut0)           # standardises 'autistic psychopathy' to 'autism'\n",
    "    stan0 = re.sub(r'(autism|autistic|asperger) syndrome', r'autism', aut1 )       # turns select 'syndrome' to 'spectrum'\n",
    "    stan1 = re.sub(r'(autism|autistic|asperger) spectrum', r'autism', stan0)       # turns select 'spectrum' to 'autism'\n",
    "    stan2 = re.sub(r'(autism|autistic|asperger) disease', r'autism', stan1)        # turns select 'disease' to 'autism'\n",
    "    stan3 = re.sub(r'(autism|autistic|asperger) disability', r'autism', stan2)     # turns select 'disability' to 'autism'\n",
    "    stan4 = re.sub(r'(autism|autistic|asperger) disorder', r'autism', stan3)       # turns select 'disorder' to 'autism'\n",
    "    stan5 = re.sub(r'(autism|autistic|asperger) \\(ASD\\)', r'autism', stan4)     # turns select former abbreviations to 'autism'\n",
    "    stan6 = re.sub(r'(autism|autistic|asperger) \\(AS\\)', r'autism', stan5)      # turns select former abbreviations to 'autism'\n",
    "    stan7 = re.sub(r'(AS)([^\\w])', r'ASD\\2 ', stan6)                  # standardises 'AS.' to 'ASD.' \n",
    "    stan8 = re.sub(r'AS ', r'ASD ', stan7)                              # standardises 'AS ' to 'ASD ' - note trailing space\n",
    "\n",
    "    return(stan8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59b747c",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Optional cell code block to test or understand what the tidy_up_terminology function does\n",
    "    \n",
    "tidy_test = \"1 Aspergers Asperger's Asperger Spectrum \\\n",
    "            2 Spectra Syndrome Syndrome \\\n",
    "            3 Disorders disorders Diseases disease ASDs ASD's \\\n",
    "            4 asperger's autism autistic psychopathy\\\n",
    "            5 autistic syndrome \\\n",
    "            6 autism disease \\\n",
    "            7 autistic disability  autism spectrum (AS)\\\n",
    "            8 asperger disability autistic disease \\\n",
    "            9 autism spectrum disorder (ASD) \\\n",
    "            10 autism spectrum (ASD) \\\n",
    "            11 autistic spectrum disease (ASD) \\\n",
    "            12 autism (ASD) \\\n",
    "            13 Asperger syndrome (AS)\\\n",
    "            14 autistic spectrum \\\n",
    "            15 AS. AS! ASQ \"\n",
    "\n",
    "tidy_up_terminology(tidy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f4b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_text = [tidy_up_terminology(abstract) for abstract in no_null_texts['Text'] ] \n",
    "                                             # create abstract list that has the terminology tidied and standardised\n",
    "                                             # this is to improve pattern recognition later\n",
    "no_null_texts['Text'] = tidy_text            # Over-write the 'Text' column with the tidied/standardised version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b020a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = no_null_texts         # A backup may be useful at this step if you want to adjust/test the tidy functions more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d31104",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_texts = backup          # If you need the backup, re-run this step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bce6eab",
   "metadata": {},
   "source": [
    "## Save the consolidated output as .csv\n",
    "\n",
    "Having imported, consolidated, tidied and checked everything, I want to save the output in a new .csv file. It is important to use a good name for the file, because bad file names are the bane of my existance. \n",
    "\n",
    "For simplicity sake, I will also create a new data frame containing only those rows for which the 'Text' column contains one of the keywords of interest, check its length and save it as a new .csv file with a good name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1cdfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(no_null_texts)                          # Let's just double check what kind of a thing 'no_null_texts' is\n",
    "                                             # This lets us know what kind of write-out-to-csv function we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc35185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_texts                                # OPTIONAL - have a quick look to see if it looks the way you expect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7147dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_texts.to_csv('..\\\\output\\\\all_abstracts_no_null_texts.csv')  # write out the data frame to a .csv, with a useful name\n",
    "                                                                     # which clarifies this is ALL abstracts with non-null texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nans_matched_texts = no_null_texts[no_null_texts['Text'].str.contains('[Aa]utis|ASD|AS|[Aa]sperger')]\n",
    "                                                         # keep only rows where text contains one or more original keywords\n",
    "len(no_nans_matched_texts)                               # check the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nans_matched_texts.to_csv('..\\\\output\\\\matched_abstracts_no_null_texts.csv')  # write out the matched texts df to a .csv too\n",
    "                                                                                 # again, with a clear and useful name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f0ce19",
   "metadata": {},
   "source": [
    "## Manually check the saved .csv files\n",
    "\n",
    "You may want to go and check that the two files you have created here have been created and saved correctly. You may even want to open them up and have a nosy through them to see what they look like. \n",
    "\n",
    "The next notebook picks up where this leaves off, by importing those files and working with them to produce some stats that help explore the research question. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
