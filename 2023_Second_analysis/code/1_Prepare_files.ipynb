{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b46e5d4f",
   "metadata": {},
   "source": [
    "# Step 1 - Prepare\n",
    "\n",
    "Admittedly, quite a bit of work has already taken place. The .pdf files were sent off to a research support team that scraped the text out of them and stored the contents of those files in .csv files which have useful headers. Each .pdf file was processed to produce a single .csv file. This processing was not universally successful because the original .pdf files were not all encoded in the same format and most had a totally unique layout. In effect, this means that each .pdf to .csv process was not equally able to automatically capture and segment the text in the .pdf into .csv columns such as author, affiliation, session code, etc. This means that some of the .csv files have empty columns while other .csv files have contents written in the same columns. This also means that some of the rows are probably faulty. Obviously, a detailed manual inspection would correct some of these errors but total accuracy is not the point of this research effort. \n",
    "\n",
    "The file for 2004 was particularly tricky and needs separate attention because the output from the research support team was not structured in a way that matches the others very well. \n",
    "\n",
    "Nevertheless, we have all the files in a somewhat useful way that allows us to use natural language processing methods to investigate person-first and identity-first language. The next step is to import the various .csv files, consolidate them in one data frame, tidy up some of the erroneous rows and columns and then same the output in a new .csv. \n",
    "\n",
    "## Get ready\n",
    "\n",
    "All of my jupyter notebooks begin with some code cells focussed on downloading/importing necessary packages, loading useful short names, and so forth. \n",
    "\n",
    "I also like to check the relevant file locations before importing the .csv files to work on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture                         \n",
    "                                  # The above capture statement is optional. \n",
    "                                  # You can remove this to see the chatter normally produced during import steps. \n",
    "\n",
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "\n",
    "import pandas as pd               # pandas is necessary for working with data frames - shortening it to pd just saves time. \n",
    "pd.set_option('display.max_colwidth', 200)   # some of the files are big so set a big column width. \n",
    "import numpy as np                # like pandas, numpy is useful and useful to have a short name for\n",
    "import statistics                 # gotsta have stats\n",
    "\n",
    "import csv                        # csv is for importing and working with csv files\n",
    "\n",
    "import re                         # things we need for RegEx corrections\n",
    "import string \n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"..\\\\results\")  )                                # check 'results' folder is not empty/has correct stuff\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88df952",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "Having got all the packages we need and having checked the files, let's import them. This requires:\n",
    "* defining a function to import multiple files from a known location (better than one-by-one importing!)\n",
    "* checking the output of the mass-import for length and contents (since I suspect 2004 may not have worked correctly)\n",
    "* having found an error, investigating it a bit\n",
    "* correcting the error by removing problem rows, manually incorporating better rows, adding correct rows back in and checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c6ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []                                                        # create empty list to hold names of files in 'results'\n",
    "def import_results(input):                                        # create a function import the contents of the\n",
    "    for f in os.listdir(input):                                   # folder named in the function input\n",
    "        f = pd.read_csv(input + '\\\\'+ f,encoding='latin1')        # by reading them in as csv files, one by one\n",
    "        files.append(f)                                           # appending the newly read csv file to a temporary list\n",
    "    output = pd.concat(files)                                     # then concatenating that temp list to the pre-defined list\n",
    "    return output                                                 # returning the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b74ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = import_results(\"..\\\\results\")      # run the newly defined function on the 'results' folder\n",
    "how_many_total = len(all_results)                # check the length \n",
    "how_many_total                                   # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10626120",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_results['Year'].drop_duplicates())     # quick check shows that 2004 (a known problem file) has not imported properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc45084",
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_no_year = all_results['Year'].isna().sum() # Let's just count how many rows NaN instead of the year\n",
    "how_many_no_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_Nan_in_Year = all_results[~all_results['Year'].isnull()]          # remove the 'Year' = Nan rows\n",
    "how_many_without_Nan_year = len(no_Nan_in_Year)                      # check length again now that Nan rows are nemoved\n",
    "how_many_without_Nan_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c7cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (how_many_total - how_many_no_year )                   # print the total number minus the number that are missing a year\n",
    "print (how_many_without_Nan_year)                            # compare to the new total just to be sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163269ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_04 = pd.read_csv('..\\\\results\\\\ESHG2004.csv')      # specifically read in year 2004 (it needed a bit of extra work)\n",
    "year_04 = year_04.iloc[:, [0,1]]                        # cut a two-column slice out of it with only the year and text\n",
    "year_04                                                 # check how it looks (which also shows us how many rows are in it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_corrected = pd.concat([no_Nan_in_Year, year_04])     # add those specially imported 2004 rows back into the output\n",
    "                                                                 # which had the weird no-year rows removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7790cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_total_new = len(all_results_corrected)                  # check length again - are we back up to where we started?\n",
    "\n",
    "if how_many_total == how_many_total_new :                        # write a quick check to be sure the totals before and after\n",
    "    print('The numbers add up!')                                 # removing/replacing the 2004 rows are the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0560fad",
   "metadata": {},
   "source": [
    "## Remove rows that imported correctly for reasons unrelated to 2004. \n",
    "\n",
    "Having imported all the various .csv files and storing them in one data frame (even that tricksy 2004 .csv) I do a bit of clean up. Turning .pdf files to .csv is not a straightforward or fool proof process, so I want to remove any rows that have nothing in the 'Text' column and check the length again to see how many we have lost. Then I want to remove any columns that are entirely empty (which is probably the result of badly imported rows that are shifted over) and have a quick look at the remaining columns and what might be in them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a5f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_texts = all_results_corrected[~all_results_corrected['Text'].isnull()] \n",
    "                                                                    # remove any rows where the 'text' column is empty\n",
    "how_many_no_null_texts = len(no_null_texts)                         # check length again - still making sense?\n",
    "print (how_many_no_null_texts)\n",
    "print (how_many_total_new - how_many_no_null_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddac587",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_texts = no_null_texts.dropna(axis=1, how=\"all\")   # remove all columns which contain only NaNs\n",
    "print(len(no_null_texts))                                 # just check the length has not changed\n",
    "no_null_texts                                             # have a nosy at which columns remain, what it is them, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bce6eab",
   "metadata": {},
   "source": [
    "## Save the consolidated output as .csv\n",
    "\n",
    "Having imported, consolidated, tidied and checked everything, I want to save the output in a new .csv file. It is important to use a good name for the file, because bad file names are the bane of my existance. \n",
    "\n",
    "For simplicity sake, I will also create a new data frame containing only those rows for which the 'Text' column contains one of the keywords of interest, check its length and save it as a new .csv file with a good name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1cdfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(no_null_texts)                          # Let's just double check what kind of a thing 'no_null_texts' is\n",
    "                                             # This lets us know what kind of write-out-to-csv function we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7147dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_texts.to_csv('..\\\\output\\\\all_abstracts_no_null_texts.csv')  # write out the data frame to a .csv, with a useful name\n",
    "                                                                     # which clarifies this is ALL abstracts with non-null texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nans_matched_texts = no_null_texts[no_null_texts['Text'].str.contains('autis|Autis|ASD|Asperger|asperger')]\n",
    "                                                         # keep only rows where text contains a keyword of interest\n",
    "len(no_nans_matched_texts)                               # check the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nans_matched_texts.to_csv('..\\\\output\\\\matched_abstracts_no_null_texts.csv')  # write out the matched texts df to a .csv too\n",
    "                                                                                 # again, with a clear and useful name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f0ce19",
   "metadata": {},
   "source": [
    "## Manually check the saved .csv files\n",
    "\n",
    "You may want to go and check that the two files you have created here have been created and saved correctly. You may even want to open them up and have a nosy through them to see what they look like. \n",
    "\n",
    "The next notebook picks up where this leaves off, by importing those files and working with them to produce some stats that help explore the research question. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
